# LLM Code Generation - Studio Comparativo

**Progetto di Tesi**: Valutazione comparativa di Large Language Models per la generazione automatica di codice

## ðŸ“‹ Panoramica del Progetto

Questo progetto confronta le prestazioni di 4 diversi LLM su task di generazione di codice:
- **GPT-4o-mini** (OpenAI via Azure)
- **Gemini 2.5 Flash-Lite** (Google)
- **Llama 3.1 8B Instant** (Meta via Groq)
- **Qwen 2.5 Coder 32B** (Alibaba via Groq)

Lo studio valuta 60 problemi di programmazione accuratamente selezionati dai dataset HumanEval e MBPP, bilanciati per:
- **Livelli di difficoltÃ **: Facile (20), Medio (20), Difficile (20)
- **Categorie**: Stringhe (15), Liste (15), Matematica (15), Algoritmi (15)

Esperimenti totali: **1.200 generazioni di codice** (60 problemi Ã— 4 modelli Ã— 5 tentativi per le metriche Pass@k)

## ðŸš€ Avvio Rapido

### Prerequisiti
- Python 3.10+
- API keys per: Google (Gemini), Groq (Llama/Qwen), Azure OpenAI (GPT)

### Installazione

1. Clona il repository:
```bash
git clone https://github.com/dimcl/llm-code-generation.git
cd llm-code-generation
```

2. Crea e attiva l'ambiente virtuale:
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. Installa le dipendenze:
```bash
pip install -r requirements.txt
```

4. Configura le API keys:
```bash
# Copia il file di esempio
cp .env.example .env

# Modifica .env e aggiungi le tue API keys
# GOOGLE_API_KEY=tua-chiave-qui
# GROQ_API_KEY=tua-chiave-qui
# AZURE_OPENAI_ENDPOINT=tuo-endpoint
# AZURE_OPENAI_KEY=tua-chiave-qui
```

### Download Dataset

```bash
python download_datasets.py
```

Questo scarica:
- HumanEval (164 problemi)
- MBPP (974 problemi)
- Subset di problemi selezionati (60 problemi)

### Esecuzione Esperimenti

**Test Pilota** (5 problemi, ~5 minuti):
```bash
python run_pilot_test.py
```

**Esperimenti Completi** (60 problemi, ~2 ore):
```bash
python run_full_experiments.py
```

### Generazione Analisi

```bash
# Genera tutte le metriche
python generate_metrics_report.py

# Genera tabelle (CSV + LaTeX)
python generate_final_tables.py

# Genera visualizzazioni
python generate_balance_figures.py
```

## Struttura del Progetto

```
llm-code-generation/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ experiments/          # Generazione ed esecuzione codice
â”‚   â”‚   â”œâ”€â”€ code_generation.py
â”‚   â”‚   â”œâ”€â”€ code_execution.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”‚   â””â”€â”€ llm_clients/      # Client API per ogni modello
â”‚   â”œâ”€â”€ evaluation/           # Calcolo metriche
â”‚   â”‚   â”œâ”€â”€ correctness_metrics.py
â”‚   â”‚   â”œâ”€â”€ quality_metrics.py
â”‚   â”‚   â”œâ”€â”€ cost_analysis.py
â”‚   â”‚   â””â”€â”€ error_classifier.py
â”‚   â”œâ”€â”€ analysis/             # Analisi statistica e visualizzazione
â”‚   â”‚   â”œâ”€â”€ statistical_tests.py
â”‚   â”‚   â”œâ”€â”€ visualization.py
â”‚   â”‚   â””â”€â”€ case_studies/
â”‚   â””â”€â”€ data/                 # Dataset
â”‚       â”œâ”€â”€ humaneval/
â”‚       â”œâ”€â”€ mbpp/
â”‚       â””â”€â”€ selected_problems/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw_outputs/          # Output grezzi dei modelli (JSON)
â”‚   â”œâ”€â”€ metrics/              # Metriche calcolate (JSON)
â”‚   â”œâ”€â”€ tables/               # Tabelle (CSV + LaTeX)
â”‚   â”œâ”€â”€ figures/              # Visualizzazioni (PNG + PDF)
â”‚   â””â”€â”€ analysis/             # Case studies e analisi qualitativa
â”œâ”€â”€ config.yaml               # Configurazione esperimenti
â”œâ”€â”€ requirements.txt          # Dipendenze Python
â””â”€â”€ README.md                 # Questo file
```

## ðŸ“Š Analisi Disponibili

### Metriche Quantitative
- **Pass@k**: Tasso di successo con k tentativi (k=1,3,5)
- **QualitÃ  del Codice**: ComplessitÃ  ciclomatica, metriche di Halstead, indice di manutenibilitÃ 
- **Efficienza**: Utilizzo token, latenza, costo per problema
- **Analisi Errori**: Classificazione di 12 tipi di errori

### Test Statistici
- Test chi-quadrato per distribuzioni categoriali
- Test H di Kruskal-Wallis per confronti non parametrici
- Test post-hoc di Dunn con correzione di Bonferroni
- Calcolo effect size (V di CramÃ©r, epsilon-quadrato)

### Visualizzazioni
- Grafici di confronto Pass@k
- Heatmap tasso di successo (difficoltÃ  Ã— categoria)
- Scatter plot costo vs accuratezza
- Barre impilate distribuzione errori
- Box plot metriche di qualitÃ 

Tutti i risultati sono disponibili in:
- `results/tables/` - 32 tabelle in formato CSV e LaTeX
- `results/figures/` - 11 figure in formato PNG e PDF
- `results/metrics/` - File JSON con metriche dettagliate

## ðŸ”¬ Metodologia

1. **Selezione Problemi**: Campionamento stratificato per garantire rappresentazione bilanciata
2. **Generazione Codice**: 5 tentativi indipendenti per problema per modello
3. **Esecuzione Sandbox**: Esecuzione isolata sicura con timeout di 10s
4. **Analisi QualitÃ **: Analisi statica usando Radon, Pylint
5. **Validazione Statistica**: Test non parametrici (p < 0.05)


- Dataset HumanEval: [OpenAI](https://github.com/openai/human-eval)
- Dataset MBPP: [Google Research](https://github.com/google-research/google-research/tree/master/mbpp)
- Provider modelli: OpenAI (Azure), Google, Meta, Alibaba
- API: Azure OpenAI, Google AI Studio, Groq
